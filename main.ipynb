{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKKCK7Jt236FXjygRqCC0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoelYanotka/text-summary-to-speech/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "zbnEZ1FIRu_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Xe8yUMte19"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers sentencepiece datasets\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "JGvNmUVP1ouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "PX9dhU-3d3eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "WO_eyQw5weI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import wikipedia\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def summarize_page(wiki_title):\n",
        "    '''\n",
        "    Retrieves a Wikipedia page with the title given in the input, and extracts\n",
        "    its sections and paragraphs. Then, it generates summaries of the paragraphs.\n",
        "    Parameters:\n",
        "        wiki_title: a string with the title of a Wikipedia page.\n",
        "    Output:\n",
        "        A list of strings with summaries of the sections of the Wikipedia page.\n",
        "    '''\n",
        "    # Get a Wikipedia page by its title\n",
        "    wiki = wikipedia.page(wiki_title)\n",
        "\n",
        "    # Get a Wikipedia page by its url and make a soup\n",
        "    source = urlopen(wiki.url).read()\n",
        "    soup = BeautifulSoup(source,'lxml')\n",
        "\n",
        "    # page will contain the list of sections in the page, as delimited by html headlines\n",
        "    # Add a list with the page title as the first element and the summary as the second\n",
        "    page = [[soup.find('h1').get_text(), wiki.summary]]\n",
        "\n",
        "    for header in soup.find_all(['h2', 'h3']):\n",
        "        header_name = header.get_text().replace('[edit]', '')\n",
        "        if header.get_text() == 'Contents':\n",
        "            continue\n",
        "        if 'References' in header.get_text():\n",
        "            break\n",
        "        # Every element in the section list will be a list with the name of the\n",
        "        # headline as the first element and the paragraphs as the next elements\n",
        "        section = [header_name]\n",
        "        for elem in header.next_siblings:\n",
        "            # Stop at next header\n",
        "            if elem.name and elem.name.startswith('h'):\n",
        "                break\n",
        "            if elem.name == 'p':\n",
        "                # re.sub eliminate references\n",
        "                paragraph = re.sub(r'\\[.*?\\]+', '', elem.get_text())\\\n",
        "                    .replace('\\n', '')\\\n",
        "                    .replace(u'\\xa0', ' ')\n",
        "                section.append(paragraph)\n",
        "        page.append(section)\n",
        "\n",
        "    print('The page is being summarized.')\n",
        "    summaries = []\n",
        "    for i, section in enumerate(page):\n",
        "        if len(section) > 1:\n",
        "            for j, paragraph in enumerate(section[1:]):\n",
        "                lenght = len(paragraph.split())\n",
        "                summary = summarizer(paragraph,\n",
        "                                     max_length=lenght,\n",
        "                                     min_length=2,\n",
        "                                     do_sample=False)\n",
        "                summaries.append(summary[0]['summary_text'])\n",
        "                print(f'\\rSection: {i+1}/{len(page)}\\tParagraph: {j+1}/{len(section)}', end='')\n",
        "    print('\\rSummary successfully completed.')\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "VPgzNRjsRJOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the model does not convert numerical characters and abbreviations into speech, the following steps are required in order for the model to be able to pronounce numbers and measurements correctly."
      ],
      "metadata": {
        "id": "OCuWsRqdLVHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import num2words\n",
        "\n",
        "def parse_numbers(text):\n",
        "    '''\n",
        "    Takes a string text as input, and returns a modified version of the input\n",
        "    string where the numbers in the text have been converted to their textual\n",
        "    form and measurement abbreviations have been replaced for their full name.\n",
        "    '''\n",
        "    # Common measurement abbreviations to replace for its full name\n",
        "    measurements = {\n",
        "    \"mm\": \"milimeters\",\n",
        "    \"cm\": \"centimeters\",\n",
        "    \"m\": \"meters\",\n",
        "    \"km\": \"kilometers\",\n",
        "    \"mg\": \"milligrams\",\n",
        "    \"g\": \"grams\",\n",
        "    \"kg\": \"kilograms\",\n",
        "    \"ml\": \"milliters\",\n",
        "    \"l\": \"liters\",\n",
        "    \"L\": \"liters\",\n",
        "    \"in\": \"inches\",\n",
        "    \"ft\": \"feet\",\n",
        "    \"yd\": \"yards\",\n",
        "    \"mi\": \"miles\",\n",
        "    \"oz\": \"ounces\",\n",
        "    \"lb\": \"pounds\",\n",
        "    \"gal\": \"gallons\"\n",
        "}\n",
        "    \n",
        "    # Find numbers folowed by any of the previous measurement abbreviations and\n",
        "    # replace the abbreviation for the full name\n",
        "    pattern = r'(\\d+)\\s*(' + '|'.join(measurements.keys()) + r')\\b'\n",
        "    text = re.sub(pattern,\n",
        "                  lambda m: m.group(1) + ' ' + measurements[m.group(2)],\n",
        "                  text)\n",
        "\n",
        "    # Eliminate the leading zeroes in a decimal number\n",
        "    text = re.sub(r\"(\\.\\d*?[1-9])0+\\b\", r\"\\1\", text)\n",
        "\n",
        "    # Find the numbers with comma separators and deletes the comma\n",
        "    matches = re.findall(r\"(\\d+,\\d+)\", text)\n",
        "    for match in matches:\n",
        "        text = text.replace(match, match.replace(\",\", \"\"))\n",
        "\n",
        "    # Find the decimal numbers and replaces the dot character for the word \"point\"\n",
        "    matches = re.findall(r\"(\\d+.\\d+)\", text)\n",
        "    for match in matches:\n",
        "        text = text.replace(match, match.replace(\".\", \" point \"))\n",
        "\n",
        "    # Replace numbers for words\n",
        "    text = re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "sGMJolR3Ih2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the text is ready to be converted into audio.\n",
        "Due to the fact that the text-to-speech model supports up to 600 words, it is neccesary to create many audio files and then combine them to create the final product."
      ],
      "metadata": {
        "id": "fiWkw3FMU7li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import soundfile as sf\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "# load xvector containing speaker's voice characteristics from a dataset\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "def text2audio(text,\n",
        "               output_name='output.wav',\n",
        "               processor=processor,\n",
        "               model=model,\n",
        "               vocoder=vocoder,\n",
        "               embeddings_dataset=embeddings_dataset,\n",
        "               speaker_embeddings=speaker_embeddings):\n",
        "    '''\n",
        "    Converts a given text into an audio file. The function uses a\n",
        "    text-to-speech model to synthesize speech and a vocoder to convert the\n",
        "    generated speech into an audio waveform.\n",
        "    '''\n",
        "    inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"],\n",
        "                                   speaker_embeddings,\n",
        "                                   vocoder=vocoder)\n",
        "    sf.write(output_name, speech.numpy(), samplerate=16000)"
      ],
      "metadata": {
        "id": "nTUcFpJix2q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae2dd99-b00a-47c6-bdcd-37602b165e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset cmu-arctic-xvectors (/root/.cache/huggingface/datasets/Matthijs___cmu-arctic-xvectors/default/0.0.1/a62fea1f9415e240301ea0042ffad2a3aadf4d1caa7f9a8d9512d631723e781f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wave\n",
        "\n",
        "def merge_audio(list_of_text, out_dir=''):\n",
        "    '''\n",
        "    Takes a list of text strings as input and outputs a merged audio file\n",
        "    containing the spoken versions of the text.\n",
        "    Parameters:\n",
        "        list_of_text (List[str]): A list of text strings to be converted to\n",
        "        speech and merged together.\n",
        "        out_dir (str): The destination directory for the audio file.\n",
        "        If not specified, it will be created in the current directory.\n",
        "    Return:\n",
        "        None. The merged audio file is saved in the specified directory.\n",
        "    '''\n",
        "    n_audios = len(list_of_text)\n",
        "    for i, line in enumerate(list_of_text):\n",
        "        text2audio(text=line, output_name=f'tmp_output_{i:03d}.wav')\n",
        "        print(f'\\r{(i+1)/n_audios*100:.0f} % of the audio file completed.', end='')\n",
        "    print('')\n",
        "\n",
        "    infiles = [f for f in os.listdir() if f.startswith('tmp_output_')]\n",
        "    infiles.sort()\n",
        "    outfile = os.path.join(os.getcwd(), out_dir, \"output.wav\")\n",
        "\n",
        "    data= []\n",
        "    for infile in infiles:\n",
        "        w = wave.open(infile, 'rb')\n",
        "        data.append([w.getparams(), w.readframes(w.getnframes())])\n",
        "        w.close()\n",
        "        \n",
        "    output = wave.open(outfile, 'wb')\n",
        "    output.setparams(data[0][0])\n",
        "    for i in range(len(data)):\n",
        "        output.writeframes(data[i][1])\n",
        "    output.close()\n",
        "\n",
        "    for f in infiles:\n",
        "        os.remove(f)\n",
        "    \n",
        "    print(f'Process completed. File path:')\n",
        "    print(f'{outfile}')"
      ],
      "metadata": {
        "id": "PgyJzvTDfNDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    wiki_title = input('Type the full title of a Wikipedia page: ')\n",
        "    wiki_title = wiki_title.strip().replace(' ', '_')\n",
        "    \n",
        "    summarized_text = summarize_page(wiki_title)\n",
        "    summarized_text = [parse_numbers(line) for line in summarized_text]\n",
        "\n",
        "    merge_audio(summarized_text)"
      ],
      "metadata": {
        "id": "mRGmcQcPW_0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
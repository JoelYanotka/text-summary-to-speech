{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORaK3VxxREdKoKMB13ydq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoelYanotka/text-summary-to-speech/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Xe8yUMte19"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "JGvNmUVP1ouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "WO_eyQw5weI0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "C19sfWixwfnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import wikipedia\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def summarize_page(wiki_title):\n",
        "    # Get a Wikipedia page by its title\n",
        "    wiki = wikipedia.page(wiki_title)\n",
        "\n",
        "    # Get a Wikipedia page by its url and make a soup\n",
        "    source = urlopen(wiki.url).read()\n",
        "    soup = BeautifulSoup(source,'lxml')\n",
        "\n",
        "    # page will contain the list of sections in the page, as delimited by html headlines\n",
        "    # Add a list with the page title as the first element and the summary as the second\n",
        "    page = [[soup.find('h1').get_text(), wiki.summary]]\n",
        "\n",
        "    for header in soup.find_all(['h2', 'h3']):\n",
        "        header_name = header.get_text().replace('[edit]', '')\n",
        "        if header.get_text() == 'Contents':\n",
        "            continue\n",
        "        if 'References' in header.get_text():\n",
        "            break\n",
        "        # Every element in the section list will be a list with the name of the\n",
        "        # headline as the first element and the paragraphs as the next elements\n",
        "        section = [header_name]\n",
        "        for elem in header.next_siblings:\n",
        "            # Stop at next header\n",
        "            if elem.name and elem.name.startswith('h'):\n",
        "                break\n",
        "            if elem.name == 'p':\n",
        "                # re.sub eliminate references\n",
        "                paragraph = re.sub(r'\\[.*?\\]+', '', elem.get_text())\\\n",
        "                    .replace('\\n', '')\\\n",
        "                    .replace(u'\\xa0', ' ')\n",
        "                section.append(paragraph)\n",
        "        page.append(section)\n",
        "\n",
        "    summaries = []\n",
        "    for i, section in enumerate(page):\n",
        "        if len(section) > 1:\n",
        "            for j, paragraph in enumerate(section[1:]):\n",
        "                lenght = len(paragraph.split())\n",
        "                summary = summarizer(paragraph, max_length=lenght, min_length=2, do_sample=False)\n",
        "                summaries.append(summary[0]['summary_text'])\n",
        "                print(f'\\rSection: {i+1}/{len(page)}\\tParagraph: {j+1} / {len(section)}', end='')\n",
        "    print('')\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "VPgzNRjsRJOt"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_title = 'Sceloporus_virgatus'\n",
        "\n",
        "summarized_text = summarize_page(wiki_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ0f7w_pSEhk",
        "outputId": "c298cebe-54fd-465a-f832-d3bde5022368"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section: 12/12\tParagraph: 1 / 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the model does not convert numerical characters and abbreviations into speech, the following steps are required in order for the model to be able to pronounce numbers and measures correctly."
      ],
      "metadata": {
        "id": "OCuWsRqdLVHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import num2words\n",
        "\n",
        "def parse_numbers(text):\n",
        "\n",
        "    # Common measure abbreviations to replace for its full name\n",
        "    measures = {\n",
        "    \"mm\": \"milimeters\",\n",
        "    \"cm\": \"centimeters\",\n",
        "    \"m\": \"meters\",\n",
        "    \"km\": \"kilometers\",\n",
        "    \"mg\": \"milligrams\",\n",
        "    \"g\": \"grams\",\n",
        "    \"kg\": \"kilograms\",\n",
        "    \"ml\": \"milliters\",\n",
        "    \"l\": \"liters\",\n",
        "    \"L\": \"liters\",\n",
        "    \"in\": \"inches\",\n",
        "    \"ft\": \"feet\",\n",
        "    \"yd\": \"yards\",\n",
        "    \"mi\": \"miles\",\n",
        "    \"oz\": \"ounces\",\n",
        "    \"lb\": \"pounds\",\n",
        "    \"gal\": \"gallons\"\n",
        "}\n",
        "    \n",
        "    # Find numbers folowed by any of the previous measure abbreviations and\n",
        "    # replace the abbreviation for the full name\n",
        "    pattern = r'(\\d+)\\s*(' + '|'.join(measures.keys()) + r')\\b'\n",
        "    text = re.sub(pattern, lambda m: m.group(1) + ' ' + measures[m.group(2)], text)\n",
        "    # Eliminate the leading zeroes in a decimal number\n",
        "    text = re.sub(r\"(\\.\\d*?[1-9])0+\\b\", r\"\\1\", text)\n",
        "\n",
        "    # Find the numbers with comma separators and deletes the comma\n",
        "    matches = re.findall(r\"(\\d+,\\d+)\", text)\n",
        "    for match in matches:\n",
        "        text = text.replace(match, match.replace(\",\", \"\"))\n",
        "\n",
        "    # Find the decimal numbers and replaces the dot character for the word \"point\"\n",
        "    matches = re.findall(r\"(\\d+.\\d+)\", text)\n",
        "    for match in matches:\n",
        "        text = text.replace(match, match.replace(\".\", \" point \"))\n",
        "\n",
        "    # Replace numbers for words\n",
        "    text = re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "sGMJolR3Ih2S"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text = [parse_numbers(line) for line in summarized_text]"
      ],
      "metadata": {
        "id": "AlXOdWyFU_A-"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the text is ready to be converted into audio"
      ],
      "metadata": {
        "id": "fiWkw3FMU7li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "def text2audio(text, output_name='output.wav', processor=processor, model=model, vocoder=vocoder):\n",
        "\n",
        "    inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "    # load xvector containing speaker's voice characteristics from a dataset\n",
        "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "    sf.write(output_name, speech.numpy(), samplerate=16000)"
      ],
      "metadata": {
        "id": "nTUcFpJix2q5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, line in enumerate(summarized_text):\n",
        "    text2audio(text=line, output_name=f'output_{i:02d}.wav')"
      ],
      "metadata": {
        "id": "PgyJzvTDfNDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wave\n",
        "\n",
        "infiles = [f for f in os.listdir() if f.startswith('output_')]\n",
        "infiles.sort()\n",
        "outfile = \"output.wav\"\n",
        "\n",
        "data= []\n",
        "for infile in infiles:\n",
        "    w = wave.open(infile, 'rb')\n",
        "    data.append([w.getparams(), w.readframes(w.getnframes())])\n",
        "    w.close()\n",
        "    \n",
        "output = wave.open(outfile, 'wb')\n",
        "output.setparams(data[0][0])\n",
        "for i in range(len(data)):\n",
        "    output.writeframes(data[i][1])\n",
        "output.close()"
      ],
      "metadata": {
        "id": "N12ctVt86C3L"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}